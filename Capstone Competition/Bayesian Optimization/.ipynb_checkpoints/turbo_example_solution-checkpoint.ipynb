{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "# from botorch.generation import MaxPosteriorSampling\n",
    "from torch.quasirandom import SobolEngine\n",
    "import gpytorch\n",
    "import botorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem\n",
    "Yield in a Chemical Reaction- This time you are trying to optimize another four-dimensional black-box. It corresponds to the yield of a chemical process after processing in some factory. This type of process tends to be unimodal. Try to find the combination of chemicals that maximizes the yield!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will see how to use the TuRBO Bayesian Optimization tool for the capstone project. TuRBO is a BO algorithm proposed by Uber that specializes in high-dimensional problems. You can read the details of the algorithm in the paper:\n",
    "\n",
    "\n",
    "Eriksson et al., \"Scalable Global Optimization via Local Bayesian Optimization\", NeurIPS (2019). URL: https://proceedings.neurips.cc/paper/2019/file/6c990b7aca7bc7058f5e98ea909e924b-Paper.pdf\n",
    "\n",
    "For implementing the method, we will be using the Gaussian Process library GPyTorch, and the Bayesian Optimization library BoTorch. We will be loosely following the tutorial made by BoTorch's team:\n",
    "\n",
    "https://botorch.org/tutorials/turbo_1\n",
    "\n",
    "However, we will be making some modification that are case-specific for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TuRBO works by creating a Trust Region over which will focus all our optimization efforts. This works great for higher-dimensions because the search space is too large and algorithms tend to over-explore! \n",
    "\n",
    "We keep track of a 'Turbo State' that dictates the size and location of the region. The code below implements a data class that will help us keep track of the state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define a dataclass for our state\n",
    "@dataclass\n",
    "class TurboState:\n",
    "    dim: int # dimension of the problem, aka input dimension\n",
    "    batch_size: int = 1 # we could do batch optimization, but the capstone only does one query at a time\n",
    "    length: float = 0.8 # the length of the current trust region\n",
    "    length_min: float = 0.5 ** 7 # minimum length for the trust region\n",
    "    length_max: float = 1.6 # maximum length for the trust region\n",
    "    failure_counter: int = 0 # initialize counter of the number of failures to improve on the best observation\n",
    "    failure_tolerance: int = float(\"nan\")  # Note: Post-initialized\n",
    "    success_counter: int = 0 # initialize counter of the number of success to improve on the best observation\n",
    "    success_tolerance: int = 10  # Note: The original paper uses 3, this is the number of successes in a row needed to expand the region\n",
    "    best_value: float = -float(\"inf\") # best value so far, initialized to be the infimum\n",
    "    restart_triggered: bool = False \n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.failure_tolerance = math.ceil(\n",
    "            max([4.0 / self.batch_size, float(self.dim) / self.batch_size]) # number of failures needed in a row to shrink the trust region\n",
    "        )\n",
    "\n",
    "\n",
    "def update_state(state, Y_next):\n",
    "    # count if a success, otherwise a failure\n",
    "    if max(Y_next) > state.best_value + 1e-3 * math.fabs(state.best_value):\n",
    "        state.success_counter += 1\n",
    "        state.failure_counter = 0\n",
    "    else:\n",
    "        state.success_counter = 0\n",
    "        state.failure_counter += 1\n",
    "    # check if we need to expand or shrink the trust region\n",
    "    if state.success_counter == state.success_tolerance:  # Expand trust region\n",
    "        state.length = min(2.0 * state.length, state.length_max)\n",
    "        state.success_counter = 0\n",
    "    elif state.failure_counter == state.failure_tolerance:  # Shrink trust region\n",
    "        state.length /= 2.0\n",
    "        state.failure_counter = 0\n",
    "    # set the best value if we got a new observation\n",
    "    state.best_value = max(state.best_value, max(Y_next).item())\n",
    "    if state.length < state.length_min:\n",
    "        state.restart_triggered = True\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will be very important to keep track of the state when we optimize, as we will need to make sure we keep the state updated from query to query. You can use a print statement to see the value of a state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TurboState(dim=4, batch_size=1, length=0.8, length_min=0.0078125, length_max=1.6, failure_counter=0, failure_tolerance=4, success_counter=0, success_tolerance=10, best_value=-inf, restart_triggered=False)\n"
     ]
    }
   ],
   "source": [
    "state = TurboState(dim = 4)\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to record these variables after choosing a new query, and re-input, and update to the correct state when we receive new observations. An example of this will be given later. We can then define the TuRBO loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(\n",
    "    state,\n",
    "    model,  # GP model\n",
    "    X,  # Evaluated points on the domain [0, 1]^d\n",
    "    Y,  # Function values\n",
    "    batch_size = 1, # fix batch size to 1\n",
    "    n_candidates=None,  # Number of candidates for Thompson sampling\n",
    "    num_restarts=10,\n",
    "    raw_samples=512,\n",
    "    acqf=\"ts\",  # \"ei\" or \"ts\"\n",
    "):\n",
    "    assert acqf in (\"ts\")\n",
    "    assert X.min() >= 0.0 and X.max() <= 1.0 and torch.all(torch.isfinite(Y))\n",
    "    if n_candidates is None:\n",
    "        n_candidates = min(5000, max(2000, 200 * X.shape[-1]))\n",
    "\n",
    "    # Scale the trust region to be proportional to the lengthscales\n",
    "    x_center = X[Y.argmax(), :].clone()\n",
    "    weights = model.covar_module.base_kernel.lengthscale.squeeze().detach()\n",
    "    weights = weights / weights.mean()\n",
    "    weights = weights / torch.prod(weights.pow(1.0 / len(weights)))\n",
    "    tr_lb = torch.clamp(x_center - weights * state.length / 2.0, 0.9, 1.0)\n",
    "    tr_ub = torch.clamp(x_center + weights * state.length / 2.0, 0.9, 1.0)\n",
    "    # we focus only on thompson sampling as an acquisition function\n",
    "    if acqf == \"ts\":\n",
    "        dim = X.shape[-1]\n",
    "        sobol = SobolEngine(dim, scramble=True)\n",
    "        pert = sobol.draw(n_candidates)\n",
    "        pert = tr_lb + (tr_ub - tr_lb) * pert\n",
    "\n",
    "        # Create a perturbation mask\n",
    "        prob_perturb = min(20.0 / dim, 1.0)\n",
    "        mask = (\n",
    "            torch.rand(n_candidates, dim)\n",
    "            <= prob_perturb\n",
    "        )\n",
    "        ind = torch.where(mask.sum(dim=1) == 0)[0]\n",
    "        mask[ind, torch.randint(0, dim - 1, size=(len(ind),))] = 1\n",
    "\n",
    "        # Create candidate points from the perturbations and the mask        \n",
    "        X_cand = x_center.expand(n_candidates, dim).clone()\n",
    "        X_cand[mask] = pert[mask]\n",
    "\n",
    "        # Sample on the candidate points\n",
    "        # set model to evaluation mode\n",
    "        model.eval()\n",
    "        posterior_distribution = model(X_cand)\n",
    "        with torch.no_grad():  # We don't need gradients when using TS\n",
    "            posterior_sample = posterior_distribution.sample()\n",
    "            X_next_idx = torch.argmax(posterior_sample)\n",
    "            X_next = X_cand[X_next_idx]\n",
    "\n",
    "    return X_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function above requires us to use a GPyTorch model as an input. A tutorial on how GPyTorch models can be used is found here: https://docs.gpytorch.ai/en/stable/examples/01_Exact_GPs/Simple_GP_Regression.html\n",
    "\n",
    "Below we define our model class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the model given in the tutorial, we also add the hyper-parameter training as a method\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        # set a constant mean\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        # use a simple RBF kernel with constant scaling\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=train_x.shape[1]))\n",
    "        # set number of hyper-parameter training iterations\n",
    "        self.training_iter = 200\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not forget to re-define our state as we have a new best-observation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "X = np.load('initial_inputs.npy')\n",
    "Y = np.load('initial_outputs.npy')\n",
    "# data = pd.read_csv('../observations.csv')\n",
    "# data = data[data['Fn'] == 5]\n",
    "# X_ = data[['X1', 'X2', 'X3', 'X4']]\n",
    "# Y_ = data['Y']\n",
    "\n",
    "# X = np.concatenate((X, X_), axis=0)\n",
    "# Y = np.append(Y, Y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.from_numpy(X)\n",
    "train_y = torch.from_numpy(Y)\n",
    "\n",
    "state = TurboState(dim = 4, best_value = torch.max(train_y).float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to train the hyper-parameters of the model. This can be done in a similar fashion to a normal PyTorch model.\n",
    "\n",
    "All we need is to define a model and a likelihood, and then activate .train() mode. We then follow classical PyTorch syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 10/200 - Loss: 461664.900   lengthscale: tensor([[0.3810, 0.3541, 0.3697, 0.3556]])   noise: 1.190\n",
      "Iter 20/200 - Loss: 293436.791   lengthscale: tensor([[0.2909, 0.1966, 0.2499, 0.2071]])   noise: 1.678\n",
      "Iter 30/200 - Loss: 227186.682   lengthscale: tensor([[0.2972, 0.1411, 0.2120, 0.1596]])   noise: 2.023\n",
      "Iter 40/200 - Loss: 192039.927   lengthscale: tensor([[0.3429, 0.1237, 0.1993, 0.1441]])   noise: 2.258\n",
      "Iter 50/200 - Loss: 168920.252   lengthscale: tensor([[0.4197, 0.1250, 0.1967, 0.1422]])   noise: 2.432\n",
      "Iter 60/200 - Loss: 152986.701   lengthscale: tensor([[0.5105, 0.1340, 0.1975, 0.1445]])   noise: 2.572\n",
      "Iter 70/200 - Loss: 141993.854   lengthscale: tensor([[0.5818, 0.1433, 0.1989, 0.1444]])   noise: 2.695\n",
      "Iter 80/200 - Loss: 133572.232   lengthscale: tensor([[0.6166, 0.1487, 0.2003, 0.1398]])   noise: 2.809\n",
      "Iter 90/200 - Loss: 126552.113   lengthscale: tensor([[0.6245, 0.1502, 0.2025, 0.1328]])   noise: 2.917\n",
      "Iter 100/200 - Loss: 120577.169   lengthscale: tensor([[0.6226, 0.1503, 0.2060, 0.1265]])   noise: 3.020\n",
      "Iter 110/200 - Loss: 115415.046   lengthscale: tensor([[0.6210, 0.1505, 0.2105, 0.1226]])   noise: 3.120\n",
      "Iter 120/200 - Loss: 110871.820   lengthscale: tensor([[0.6222, 0.1512, 0.2155, 0.1209]])   noise: 3.218\n",
      "Iter 130/200 - Loss: 106824.913   lengthscale: tensor([[0.6248, 0.1519, 0.2203, 0.1204]])   noise: 3.314\n",
      "Iter 140/200 - Loss: 103189.600   lengthscale: tensor([[0.6274, 0.1524, 0.2245, 0.1200]])   noise: 3.407\n",
      "Iter 150/200 - Loss: 99898.677   lengthscale: tensor([[0.6293, 0.1525, 0.2280, 0.1195]])   noise: 3.499\n",
      "Iter 160/200 - Loss: 96899.583   lengthscale: tensor([[0.6307, 0.1525, 0.2310, 0.1190]])   noise: 3.590\n",
      "Iter 170/200 - Loss: 94150.915   lengthscale: tensor([[0.6317, 0.1525, 0.2335, 0.1184]])   noise: 3.678\n",
      "Iter 180/200 - Loss: 91619.160   lengthscale: tensor([[0.6327, 0.1526, 0.2357, 0.1180]])   noise: 3.766\n",
      "Iter 190/200 - Loss: 89276.824   lengthscale: tensor([[0.6335, 0.1526, 0.2376, 0.1177]])   noise: 3.852\n",
      "Iter 200/200 - Loss: 87101.088   lengthscale: tensor([[0.6342, 0.1527, 0.2393, 0.1174]])   noise: 3.936\n"
     ]
    }
   ],
   "source": [
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(train_x, train_y, likelihood)\n",
    "\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "\n",
    "for i in range(model.training_iter):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(train_x)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    if i % 10 == 9:\n",
    "        print(f'Iter %d/%d - Loss: %.3f   lengthscale: {model.covar_module.base_kernel.lengthscale.detach()}   noise: %.3f' % (\n",
    "            i + 1, model.training_iter, loss.item(),\n",
    "            model.likelihood.noise.item()\n",
    "        ))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define a function that takes as input:\n",
    "1. Training Data\n",
    "2. A TuRBO State\n",
    "\n",
    "And returns the next suggested query! We will define the GP model and optimize the GP's hyper-parameters inside the function itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_query_via_TurBO(train_x, train_y, turbo_state, verbose = False):\n",
    "    # the verbose variable decides wether to print the hyper-parameter optimization details\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = ExactGPModel(train_x, train_y, likelihood)\n",
    "\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "\n",
    "    for i in range(model.training_iter):\n",
    "        # Zero gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Output from model\n",
    "        output = model(train_x)\n",
    "        # Calc loss and backprop gradients\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        if (i % 10 == 9) & verbose:\n",
    "            print(f'Iter %d/%d - Loss: %.3f   lengthscale: {model.covar_module.base_kernel.lengthscale}   noise: %.3f' % (\n",
    "                i + 1, model.training_iter, loss.item(),\n",
    "                model.likelihood.noise.item()\n",
    "            ))\n",
    "        optimizer.step()\n",
    "    \n",
    "    return generate_batch(turbo_state, model = model, X = train_x, Y = train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can obtain a suggested query!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next chose query: tensor([0.9924, 0.9945, 0.9990, 0.9911], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mijin/anaconda3/lib/python3.11/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-08 to the diagonal\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "next_query = next_query_via_TurBO(train_x=train_x, train_y=train_y, turbo_state=state)\n",
    "print(f'Next chose query: {next_query}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to keep track of the state, and also update it when we receive new information. For example, the state that was used to choose the query can be printed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State at latest optimization loop:\n",
      "\n",
      "TurboState(dim=4, batch_size=1, length=0.8, length_min=0.0078125, length_max=1.6, failure_counter=0, failure_tolerance=4, success_counter=0, success_tolerance=10, best_value=tensor(8662.4053), restart_triggered=False)\n"
     ]
    }
   ],
   "source": [
    "print('State at latest optimization loop:')\n",
    "print()\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TurboState(dim=4, batch_size=1, length=0.8, length_min=0.0078125, length_max=1.6, failure_counter=0, failure_tolerance=4, success_counter=0, success_tolerance=10, best_value=tensor(8662.4053), restart_triggered=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what observation we would have gotten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8242.5791])\n"
     ]
    }
   ],
   "source": [
    "y_next =  torch.tensor([8242.578813191945])\n",
    "print(y_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a new observation, it is vital to update the state before optimizing again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state = update_state(state, y_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now optimize again to obtain the next point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New query: tensor([0.9579, 0.9941, 0.9951, 0.9952], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "train_x = torch.concat((train_x, next_query.reshape(1, -1)), dim = 0)\n",
    "train_y = torch.concat((train_y, y_next))\n",
    "\n",
    "next_next_query = next_query_via_TurBO(train_x, train_y, new_state)\n",
    "print(f'New query:', next_next_query)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6df83ed6fabd41a8e562c5a64e44b5d97b19c29150cbf5eb47fd88445500a37c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

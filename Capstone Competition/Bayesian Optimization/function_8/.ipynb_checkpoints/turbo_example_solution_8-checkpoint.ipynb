{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "# from botorch.generation import MaxPosteriorSampling\n",
    "from torch.quasirandom import SobolEngine\n",
    "import gpytorch\n",
    "import botorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will see how to use the TuRBO Bayesian Optimization tool for the capstone project. TuRBO is a BO algorithm proposed by Uber that specializes in high-dimensional problems. You can read the details of the algorithm in the paper:\n",
    "\n",
    "\n",
    "Eriksson et al., \"Scalable Global Optimization via Local Bayesian Optimization\", NeurIPS (2019). URL: https://proceedings.neurips.cc/paper/2019/file/6c990b7aca7bc7058f5e98ea909e924b-Paper.pdf\n",
    "\n",
    "For implementing the method, we will be using the Gaussian Process library GPyTorch, and the Bayesian Optimization library BoTorch. We will be loosely following the tutorial made by BoTorch's team:\n",
    "\n",
    "https://botorch.org/tutorials/turbo_1\n",
    "\n",
    "However, we will be making some modification that are case-specific for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TuRBO works by creating a Trust Region over which will focus all our optimization efforts. This works great for higher-dimensions because the search space is too large and algorithms tend to over-explore! \n",
    "\n",
    "We keep track of a 'Turbo State' that dictates the size and location of the region. The code below implements a data class that will help us keep track of the state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define a dataclass for our state\n",
    "@dataclass\n",
    "class TurboState:\n",
    "    dim: int # dimension of the problem, aka input dimension\n",
    "    batch_size: int = 1 # we could do batch optimization, but the capstone only does one query at a time\n",
    "    length: float = 0.8 # the length of the current trust region\n",
    "    length_min: float = 0.5 ** 7 # minimum length for the trust region\n",
    "    length_max: float = 1.6 # maximum length for the trust region\n",
    "    failure_counter: int = 0 # initialize counter of the number of failures to improve on the best observation\n",
    "    failure_tolerance: int = float(\"nan\")  # Note: Post-initialized\n",
    "    success_counter: int = 0 # initialize counter of the number of success to improve on the best observation\n",
    "    success_tolerance: int = 10  # Note: The original paper uses 3, this is the number of successes in a row needed to expand the region\n",
    "    best_value: float = -float(\"inf\") # best value so far, initialized to be the infimum\n",
    "    restart_triggered: bool = False \n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.failure_tolerance = math.ceil(\n",
    "            max([4.0 / self.batch_size, float(self.dim) / self.batch_size]) # number of failures needed in a row to shrink the trust region\n",
    "        )\n",
    "\n",
    "\n",
    "def update_state(state, Y_next):\n",
    "    # count if a success, otherwise a failure\n",
    "    if max(Y_next) > state.best_value + 1e-3 * math.fabs(state.best_value):\n",
    "        state.success_counter += 1\n",
    "        state.failure_counter = 0\n",
    "    else:\n",
    "        state.success_counter = 0\n",
    "        state.failure_counter += 1\n",
    "    # check if we need to expand or shrink the trust region\n",
    "    if state.success_counter == state.success_tolerance:  # Expand trust region\n",
    "        state.length = min(2.0 * state.length, state.length_max)\n",
    "        state.success_counter = 0\n",
    "    elif state.failure_counter == state.failure_tolerance:  # Shrink trust region\n",
    "        state.length /= 2.0\n",
    "        state.failure_counter = 0\n",
    "    # set the best value if we got a new observation\n",
    "    state.best_value = max(state.best_value, max(Y_next).item())\n",
    "    if state.length < state.length_min:\n",
    "        state.restart_triggered = True\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will be very important to keep track of the state when we optimize, as we will need to make sure we keep the state updated from query to query. You can use a print statement to see the value of a state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TurboState(dim=6, batch_size=1, length=0.8, length_min=0.0078125, length_max=1.6, failure_counter=0, failure_tolerance=6, success_counter=0, success_tolerance=10, best_value=-inf, restart_triggered=False)\n"
     ]
    }
   ],
   "source": [
    "state = TurboState(dim = 6)\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to record these variables after choosing a new query, and re-input, and update to the correct state when we receive new observations. An example of this will be given later. We can then define the TuRBO loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(\n",
    "    state,\n",
    "    model,  # GP model\n",
    "    X,  # Evaluated points on the domain [0, 1]^d\n",
    "    Y,  # Function values\n",
    "    batch_size = 1, # fix batch size to 1\n",
    "    n_candidates=None,  # Number of candidates for Thompson sampling\n",
    "    num_restarts=10,\n",
    "    raw_samples=512,\n",
    "    acqf=\"ts\",  # \"ei\" or \"ts\"\n",
    "):\n",
    "    assert acqf in (\"ts\")\n",
    "    assert X.min() >= 0.0 and X.max() <= 1.0 and torch.all(torch.isfinite(Y))\n",
    "    if n_candidates is None:\n",
    "        n_candidates = min(5000, max(2000, 200 * X.shape[-1]))\n",
    "\n",
    "    # Scale the trust region to be proportional to the lengthscales\n",
    "    x_center = X[Y.argmax(), :].clone()\n",
    "    weights = model.covar_module.base_kernel.lengthscale.squeeze().detach()\n",
    "    weights = weights / weights.mean()\n",
    "    weights = weights / torch.prod(weights.pow(1.0 / len(weights)))\n",
    "    tr_lb = torch.clamp(x_center - weights * state.length / 2.0, 0.0, 1.0)\n",
    "    tr_ub = torch.clamp(x_center + weights * state.length / 2.0, 0.0, 1.0)\n",
    "    # we focus only on thompson sampling as an acquisition function\n",
    "    if acqf == \"ts\":\n",
    "        dim = X.shape[-1]\n",
    "        sobol = SobolEngine(dim, scramble=True)\n",
    "        pert = sobol.draw(n_candidates)\n",
    "        pert = tr_lb + (tr_ub - tr_lb) * pert\n",
    "\n",
    "        # Create a perturbation mask\n",
    "        prob_perturb = min(20.0 / dim, 1.0)\n",
    "        mask = (\n",
    "            torch.rand(n_candidates, dim)\n",
    "            <= prob_perturb\n",
    "        )\n",
    "        ind = torch.where(mask.sum(dim=1) == 0)[0]\n",
    "        mask[ind, torch.randint(0, dim - 1, size=(len(ind),))] = 1\n",
    "\n",
    "        # Create candidate points from the perturbations and the mask        \n",
    "        X_cand = x_center.expand(n_candidates, dim).clone()\n",
    "        X_cand[mask] = pert[mask]\n",
    "\n",
    "        # Sample on the candidate points\n",
    "        # set model to evaluation mode\n",
    "        model.eval()\n",
    "        posterior_distribution = model(X_cand)\n",
    "        with torch.no_grad():  # We don't need gradients when using TS\n",
    "            posterior_sample = posterior_distribution.sample()\n",
    "            X_next_idx = torch.argmax(posterior_sample)\n",
    "            X_next = X_cand[X_next_idx]\n",
    "\n",
    "    return X_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function above requires us to use a GPyTorch model as an input. A tutorial on how GPyTorch models can be used is found here: https://docs.gpytorch.ai/en/stable/examples/01_Exact_GPs/Simple_GP_Regression.html\n",
    "\n",
    "Below we define our model class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the model given in the tutorial, we also add the hyper-parameter training as a method\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        # set a constant mean\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        # use a simple RBF kernel with constant scaling\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=train_x.shape[1]))\n",
    "        # set number of hyper-parameter training iterations\n",
    "        self.training_iter = 200\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this notebook, we will optimize the function:\n",
    "\n",
    "$$f(x_1, x_2, x_3, x_4, x_5, x_6) = x_3 * \\sin(x_1) * \\cos(x_2) + x_4 * x_5 - x_6 * x_5^2$$\n",
    "\n",
    "We will create an initial data set at random.\n",
    "\n",
    "Do not forget to re-define our state as we have a new best-observation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = lambda x: torch.sin(x[:, 0]) * torch.cos(x[:, 1]) * x[:, 2] + x[:, 3] * x[:, 4] - x[:, 5] * (x[:, 4]**2)\n",
    "\n",
    "train_x = torch.rand(size = torch.Size([15, 6]))\n",
    "train_y = func(train_x)\n",
    "\n",
    "state = TurboState(dim = 6, best_value = torch.max(train_y).float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to train the hyper-parameters of the model. This can be done in a similar fashion to a normal PyTorch model.\n",
    "\n",
    "All we need is to define a model and a likelihood, and then activate .train() mode. We then follow classical PyTorch syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 10/200 - Loss: 0.628   lengthscale: tensor([[1.2273, 1.2266, 1.2295, 1.2010, 1.2306, 1.2237]])   noise: 0.339\n",
      "Iter 20/200 - Loss: 0.263   lengthscale: tensor([[1.8851, 1.8657, 1.8732, 1.3837, 1.8691, 1.8308]])   noise: 0.136\n",
      "Iter 30/200 - Loss: 0.043   lengthscale: tensor([[2.4546, 2.3514, 2.2916, 0.7824, 2.2390, 2.1815]])   noise: 0.055\n",
      "Iter 40/200 - Loss: -0.001   lengthscale: tensor([[2.8851, 2.5727, 2.2025, 0.3485, 2.2286, 1.9627]])   noise: 0.030\n",
      "Iter 50/200 - Loss: -0.056   lengthscale: tensor([[3.0497, 2.6255, 1.4131, 0.4364, 1.8460, 1.1564]])   noise: 0.024\n",
      "Iter 60/200 - Loss: -0.089   lengthscale: tensor([[3.0236, 2.7670, 0.6266, 0.6209, 1.4572, 0.6679]])   noise: 0.021\n",
      "Iter 70/200 - Loss: -0.113   lengthscale: tensor([[2.7905, 3.0980, 0.5513, 0.4814, 1.5055, 0.7998]])   noise: 0.016\n",
      "Iter 80/200 - Loss: -0.123   lengthscale: tensor([[2.2130, 3.3980, 0.5483, 0.4487, 1.2917, 0.6082]])   noise: 0.013\n",
      "Iter 90/200 - Loss: -0.153   lengthscale: tensor([[1.1808, 3.6585, 0.5961, 0.4967, 0.9678, 0.6609]])   noise: 0.011\n",
      "Iter 100/200 - Loss: -0.194   lengthscale: tensor([[0.7447, 3.9387, 0.9303, 0.5002, 0.6206, 0.7536]])   noise: 0.008\n",
      "Iter 110/200 - Loss: -0.236   lengthscale: tensor([[0.9410, 4.2686, 1.2958, 0.6204, 0.5720, 0.8005]])   noise: 0.005\n",
      "Iter 120/200 - Loss: -0.273   lengthscale: tensor([[0.9823, 4.7210, 1.4530, 0.7826, 0.5951, 0.9670]])   noise: 0.003\n",
      "Iter 130/200 - Loss: -0.302   lengthscale: tensor([[1.0318, 5.3436, 1.6395, 0.8687, 0.6224, 0.9920]])   noise: 0.002\n",
      "Iter 140/200 - Loss: -0.323   lengthscale: tensor([[1.1127, 6.0607, 1.7724, 0.9595, 0.6411, 1.0335]])   noise: 0.001\n",
      "Iter 150/200 - Loss: -0.339   lengthscale: tensor([[1.1404, 6.7980, 1.8539, 1.0428, 0.6557, 1.1055]])   noise: 0.001\n",
      "Iter 160/200 - Loss: -0.350   lengthscale: tensor([[1.1827, 7.5154, 1.9512, 1.0939, 0.6681, 1.1366]])   noise: 0.001\n",
      "Iter 170/200 - Loss: -0.358   lengthscale: tensor([[1.2176, 8.1924, 2.0213, 1.1489, 0.6803, 1.1566]])   noise: 0.001\n",
      "Iter 180/200 - Loss: -0.364   lengthscale: tensor([[1.2408, 8.8251, 2.0747, 1.1855, 0.6881, 1.1848]])   noise: 0.001\n",
      "Iter 190/200 - Loss: -0.369   lengthscale: tensor([[1.2586, 9.4138, 2.1187, 1.2153, 0.6939, 1.2041]])   noise: 0.000\n",
      "Iter 200/200 - Loss: -0.372   lengthscale: tensor([[1.2751, 9.9607, 2.1484, 1.2389, 0.6984, 1.2167]])   noise: 0.000\n"
     ]
    }
   ],
   "source": [
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(train_x, train_y, likelihood)\n",
    "\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "\n",
    "for i in range(model.training_iter):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(train_x)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    if i % 10 == 9:\n",
    "        print(f'Iter %d/%d - Loss: %.3f   lengthscale: {model.covar_module.base_kernel.lengthscale.detach()}   noise: %.3f' % (\n",
    "            i + 1, model.training_iter, loss.item(),\n",
    "            model.likelihood.noise.item()\n",
    "        ))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define a function that takes as input:\n",
    "1. Training Data\n",
    "2. A TuRBO State\n",
    "\n",
    "And returns the next suggested query! We will define the GP model and optimize the GP's hyper-parameters inside the function itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_query_via_TurBO(train_x, train_y, turbo_state, verbose = False):\n",
    "    # the verbose variable decides wether to print the hyper-parameter optimization details\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = ExactGPModel(train_x, train_y, likelihood)\n",
    "\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "\n",
    "    for i in range(model.training_iter):\n",
    "        # Zero gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Output from model\n",
    "        output = model(train_x)\n",
    "        # Calc loss and backprop gradients\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        if (i % 10 == 9) & verbose:\n",
    "            print(f'Iter %d/%d - Loss: %.3f   lengthscale: {model.covar_module.base_kernel.lengthscale}   noise: %.3f' % (\n",
    "                i + 1, model.training_iter, loss.item(),\n",
    "                model.likelihood.noise.item()\n",
    "            ))\n",
    "        optimizer.step()\n",
    "    \n",
    "    return generate_batch(turbo_state, model = model, X = train_x, Y = train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can obtain a suggested query!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next chose query: tensor([0.7422, 0.2740, 0.1775, 0.9742, 0.5863, 0.3358])\n"
     ]
    }
   ],
   "source": [
    "next_query = next_query_via_TurBO(train_x=train_x, train_y=train_y, turbo_state=state)\n",
    "print(f'Next chose query: {next_query}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to keep track of the state, and also update it when we receive new information. For example, the state that was used to choose the query can be printed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State at latest optimization loop:\n",
      "\n",
      "TurboState(dim=6, batch_size=1, length=0.8, length_min=0.0078125, length_max=1.6, failure_counter=0, failure_tolerance=6, success_counter=0, success_tolerance=10, best_value=tensor(0.3943), restart_triggered=False)\n"
     ]
    }
   ],
   "source": [
    "print('State at latest optimization loop:')\n",
    "print()\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what observation we would have gotten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5712])\n"
     ]
    }
   ],
   "source": [
    "y_next = func(next_query.reshape(1, -1))\n",
    "print(y_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a new observation, it is vital to update the state before optimizing again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state = update_state(state, y_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now optimize again to obtain the next point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New query: tensor([0.9380, 0.9324, 0.1911, 0.8467, 0.0329, 0.0635])\n"
     ]
    }
   ],
   "source": [
    "train_x = torch.concat((train_x, next_query.reshape(1, -1)), dim = 0)\n",
    "train_y = torch.concat((train_y, y_next))\n",
    "\n",
    "next_next_query = next_query_via_TurBO(train_x, train_y, new_state)\n",
    "print(f'New query:', next_next_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you feel TuRBO would help you with the high-dimensional problems in the Capstone, give it a go! There a few questions that may lead to better performance:\n",
    "\n",
    "1. Maybe you can constraint some of the GPs hyper-parameters for better behaviour?\n",
    "2. How are you planning to initialize the Turbo State for the first time?\n",
    "\n",
    "So take the code from this notebook and modify it to your liking!"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6df83ed6fabd41a8e562c5a64e44b5d97b19c29150cbf5eb47fd88445500a37c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
